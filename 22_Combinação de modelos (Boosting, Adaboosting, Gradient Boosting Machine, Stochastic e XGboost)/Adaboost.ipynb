{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda5e29b",
   "metadata": {},
   "source": [
    "# <code style=\"color:red\">Adaboost:</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df132ce",
   "metadata": {},
   "source": [
    "Random Forest Algorithm é um algoritmo de aprendizado de máquina comumente usado que combina a saída de várias árvores de decisão para obter um único resultado. Ele lida com problemas de classificação e regressão, pois combina a simplicidade das árvores de decisão com flexibilidade, levando a melhorias significativas na precisão.\n",
    "\n",
    "Algoritmo AdaBoost (Adaptive Boosting) , é um método de Boosting usado como um Ensemble Machine Learning System. Os pesos de cada árvore são redistribuídos a cada turno, com pesos maiores atribuídos a condições incorretamente classificadas, por isso é chamado de Adaptive Boosting. O AdaBoost usa várias árvores de tomada de decisão de nível único chamadas de Floresta das Árvores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13456dd",
   "metadata": {},
   "source": [
    "### <code style=\"color:blue\">Tamanho da árvore:</code>\n",
    "\n",
    "RF: Usa uma árvore de decisão de tamanho normal sem tamanho de profundidade predeterminado.\n",
    "\n",
    "AB: combina muitos “weak learners (aprendizes fracos)” para fazer classificações. Os alunos fracos são quase sempre tocos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d1f6c0",
   "metadata": {},
   "source": [
    "### <code style=\"color:blue\">Distribuição de dizer para cada árvore:</code>\n",
    "\n",
    "RF: Cada árvore tem um voto igual na classificação final.\n",
    "\n",
    "AB: Alguns tocos têm mais influência na classificação do que outros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dda26e2",
   "metadata": {},
   "source": [
    "### <code style=\"color:blue\">Precisão de classificação:</code>\n",
    "Precisão de classificação:\n",
    "\n",
    "RF: O Random Forest geralmente é superado pelo AdaBoost em termos de precisão de classificação.\n",
    "\n",
    "AB: O AdaBoost costuma ser muito melhor em fazer classificações precisas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff396552",
   "metadata": {},
   "source": [
    "### <code style=\"color:blue\">Ordem de Construção de Árvore:</code>\n",
    "\n",
    "RF:Cada árvore de decisão é feita independentemente das outras\n",
    "\n",
    "AB:Cada toco é feito levando em consideração os erros do toco anterior.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a9c352",
   "metadata": {},
   "source": [
    "### <code style=\"color:blue\">Tolerância de sobreajuste:</code>\n",
    "\n",
    "\n",
    "RF: A Random Forest é menos sensível ao overfitting em comparação com o AdaBoost.\n",
    "\n",
    "AB: O Adaboost também é menos tolerante ao overfitting do que o Random Forest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887c772f",
   "metadata": {},
   "source": [
    "### <code style=\"color:blue\">Técnica de Amostragem de Dados:</code>\n",
    "\n",
    "\n",
    "RF: Os dados de treinamento são amostrados com base na técnica de ensacamento.\n",
    "\n",
    "AB: Adaboost é baseado na técnica de boosting. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08e8516",
   "metadata": {},
   "source": [
    "### <code style=\"color:blue\">Cálculo Estimado:</code>\n",
    "\n",
    "\n",
    "RF: Visa diminuir a variância e não o viés.\n",
    "\n",
    "AB: Visa diminuir o viés, não a variância.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab6347f",
   "metadata": {},
   "source": [
    "### <code style=\"color:blue\">Operação de combinação:</code>\n",
    "\n",
    "\n",
    "RF: Random Forest emprega montagem paralela. O Forest processa árvores em paralelo, permitindo que os trabalhos sejam paralelizados em uma máquina multiprocessadora.\n",
    "\n",
    "AB: Adaboost faz uso de agrupamento sequencial. É preciso um método passo a passo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9225d5e1",
   "metadata": {},
   "source": [
    "# <code style=\"color:red\">Exemplo 01 de Adaboost:</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "576734b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466666666666665"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.model_selection import cross_val_score\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    ">>> X, y = load_iris(return_X_y=True)\n",
    ">>> clf = AdaBoostClassifier(n_estimators=100)\n",
    ">>> scores = cross_val_score(clf, X, y, cv=5)\n",
    ">>> scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9e593",
   "metadata": {},
   "source": [
    "# <code style=\"color:red\">Exemplo 02 de Adaboost:</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feead48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9111111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucca\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load libraries\n",
    "from sklearn import datasets\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Import Support Vector Classifier\n",
    "from sklearn.svm import SVC\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "svc=SVC(probability=True, kernel='linear')\n",
    "\n",
    "# Create adaboost classifer object\n",
    "abc =AdaBoostClassifier(n_estimators=100, base_estimator=svc,learning_rate=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Train Adaboost Classifer\n",
    "model = abc.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4f4a3a",
   "metadata": {},
   "source": [
    "# <code style=\"color:red\">Hyperparameters:</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbabdc22",
   "metadata": {},
   "source": [
    "estimatorobject, n_estimators, learning_rate, algorithm, random_state, base_estimator "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39cb773",
   "metadata": {},
   "source": [
    "# <code style=\"color:red\">Grid Search:</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae427edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.948889 using {'learning_rate': 0.0001, 'n_estimators': 500}\n",
      "0.666667 (0.000000) with: {'learning_rate': 0.0001, 'n_estimators': 10}\n",
      "0.666667 (0.000000) with: {'learning_rate': 0.0001, 'n_estimators': 50}\n",
      "0.666667 (0.000000) with: {'learning_rate': 0.0001, 'n_estimators': 100}\n",
      "0.948889 (0.053564) with: {'learning_rate': 0.0001, 'n_estimators': 500}\n",
      "0.666667 (0.000000) with: {'learning_rate': 0.001, 'n_estimators': 10}\n",
      "0.948889 (0.053564) with: {'learning_rate': 0.001, 'n_estimators': 50}\n",
      "0.948889 (0.053564) with: {'learning_rate': 0.001, 'n_estimators': 100}\n",
      "0.922222 (0.048939) with: {'learning_rate': 0.001, 'n_estimators': 500}\n",
      "0.948889 (0.053564) with: {'learning_rate': 0.01, 'n_estimators': 10}\n",
      "0.922222 (0.048939) with: {'learning_rate': 0.01, 'n_estimators': 50}\n",
      "0.922222 (0.048939) with: {'learning_rate': 0.01, 'n_estimators': 100}\n",
      "0.926667 (0.049740) with: {'learning_rate': 0.01, 'n_estimators': 500}\n",
      "0.922222 (0.048939) with: {'learning_rate': 0.1, 'n_estimators': 10}\n",
      "0.922222 (0.048939) with: {'learning_rate': 0.1, 'n_estimators': 50}\n",
      "0.922222 (0.048939) with: {'learning_rate': 0.1, 'n_estimators': 100}\n",
      "0.917778 (0.044500) with: {'learning_rate': 0.1, 'n_estimators': 500}\n",
      "0.944444 (0.054659) with: {'learning_rate': 1.0, 'n_estimators': 10}\n",
      "0.946667 (0.055511) with: {'learning_rate': 1.0, 'n_estimators': 50}\n",
      "0.940000 (0.062893) with: {'learning_rate': 1.0, 'n_estimators': 100}\n",
      "0.940000 (0.062893) with: {'learning_rate': 1.0, 'n_estimators': 500}\n",
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# example of grid searching key hyperparameters for adaboost on a classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# define the model with default hyperparameters\n",
    "model = AdaBoostClassifier()\n",
    "\n",
    "# define the grid of values to search\n",
    "grid = dict()\n",
    "grid['n_estimators'] = [10, 50, 100, 500]\n",
    "grid['learning_rate'] = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')\n",
    "\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(X, y)\n",
    "\n",
    "# summarize the best score and configuration\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# summarize all scores that were evaluated\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca6a72a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepeatedStratifiedKFold(n_repeats=3, n_splits=10, random_state=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad10470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
